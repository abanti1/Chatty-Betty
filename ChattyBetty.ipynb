{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Final Project for Abanti Ghosh enrolled in April'23 Batch of Artificial Intelligence*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Assigned: Chatbot using TF-IDF and Cosine Similarity*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K3ArmyFGH-uV"
      },
      "source": [
        "# ChattyBetty"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "49ZiQcNpKUIq"
      },
      "source": [
        "This is a chatbot named ChattyBetty. ChattyBetty employs TF-IDF and Cosine Similarity to process user input."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ugx5CL7J66X"
      },
      "source": [
        "TF-IDF stands for Term Frequency Inverse Document Frequency of records. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus (data-set).\n",
        "\n",
        "**Term Frequency:** In document d, the frequency represents the number of instances of a given word t. Therefore, we can see that it becomes more relevant when a word appears in the text, which is rational. Since the ordering of terms is not significant, we can use a vector to describe the text in the bag of term models. For each specific term in the paper, there is an entry with the value being the term frequency.\n",
        "\n",
        "The weight of a term that occurs in a document is simply proportional to the term frequency.\n",
        "\n",
        "        tf(t,d) = count of t in d / number of words in d\n",
        "\n",
        "**Document Frequency:** This tests the meaning of the text, which is very similar to TF, in the whole corpus collection. The only difference is that in document d, TF is the frequency counter for a term t, while df is the number of occurrences in the document set N of the term t. In other words, the number of papers in which the word is present is DF.\n",
        "\n",
        "        df(t) = occurrence of t in documents\n",
        "\n",
        "**Inverse Document Frequency:** Mainly, it tests how relevant the word is. The key aim of the search is to locate the appropriate records that fit the demand. Since tf considers all terms equally significant, it is therefore not only possible to use the term frequencies to measure the weight of the term in the paper. First, we find the document frequency of a term t by counting the number of documents containing the term:\n",
        "\n",
        "        df(t) = N(t)\n",
        "        where\n",
        "        df(t) = Document frequency of a term t\n",
        "        N(t) = Number of documents containing the term t\n",
        "        idf(t) = N/ df(t) = N/N(t)\n",
        "\n",
        "Putting it together:\n",
        "\n",
        "        tf-idf(t, d) = tf(t, d) * idf(t)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dcZJHvzqJ--0"
      },
      "source": [
        "Cosine similarity is a metric, helpful in determining, how similar the data objects are irrespective of their size. We can measure the similarity between two sentences in Python using Cosine Similarity. In cosine similarity, data objects in a dataset are treated as a vector. The formula to find the cosine similarity between two vectors is –\n",
        "\n",
        "        Cos(x, y) = x . y / ||x|| * ||y||\n",
        "\n",
        "        where\n",
        "\n",
        "        x . y = product (dot) of the vectors ‘x’ and ‘y’.\n",
        "        ||x|| and ||y|| = length of the two vectors ‘x’ and ‘y’.\n",
        "        ||x|| * ||y|| = cross product of the two vectors ‘x’ and ‘y’.\n",
        "\n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".\n",
        ".\n",
        "."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-V9tuKTxKCTJ"
      },
      "source": [
        "ChattyBetty uses TF-IDF vectorizer from the scikit-learn library to compute the document vectors. The cosine_similarity function calculates the cosine similarity scores between the user query vector and the document vectors. The response with the highest similarity score is then selected and returned as ChattyBetty's reply."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-BeJaTeDKFwY"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_6Hn123LYnD"
      },
      "outputs": [],
      "source": [
        "#importing allnecessary libraries\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import urllib\n",
        "from urllib.request import urlopen\n",
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "#decoding an online text document\n",
        "#this document contains chat messages that the bot will be trained on\n",
        "\n",
        "file_url=\"https://github.com/abanti1/Text_Corpus/raw/main/Text%20Corpus%202.txt\"\n",
        "text_file=urlopen(file_url).read().decode('utf-16le')\n",
        "\n",
        "#separating the file into individual tokens or sentences\n",
        "\n",
        "corpus = sent_tokenize(text_file)\n",
        "\n",
        "# initialize the TF-IDF vectorizer\n",
        "\n",
        "vec = TfidfVectorizer()\n",
        "vec.fit(corpus)\n",
        "\n",
        "#this function processes user input and generates a corresponding output\n",
        "\n",
        "def bettys_response(input_text):\n",
        "\n",
        "    # preprocess user input\n",
        "\n",
        "    input_vector = vec.transform([input_text])\n",
        "    output_vector = vec.transform(corpus)\n",
        "\n",
        "    # compute cosine similarity between input and output\n",
        "\n",
        "    similarities = cosine_similarity(input_vector, output_vector)\n",
        "\n",
        "    # find index of response with highest similarity score\n",
        "\n",
        "    most_similar_index = np.argmax(similarities)\n",
        "\n",
        "    # return the corresponding output\n",
        "\n",
        "    return corpus[most_similar_index]\n",
        "\n",
        "# introductory message\n",
        "\n",
        "print(\"Hello, this is ChattyBetty!\")\n",
        "\n",
        "# interaction with the bot\n",
        "\n",
        "flag=True\n",
        "\n",
        "while (flag==True):\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # 'bye' ends the conversation\n",
        "\n",
        "    if(user_input.lower()=='bye'):\n",
        "      flag=False\n",
        "\n",
        "    response = bettys_response(user_input)\n",
        "    print(\"ChattyBetty:\", response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
